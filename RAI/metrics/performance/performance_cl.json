{
    "name" : "performance_cl",
    "display_name": "Classification Performance Metrics",
    "compatibility": {"task_type": "classification",
                      "data_type": ["numeric"],
                      "output_requirements": ["predict"],
                      "dataset_requirements": ["y"]},
    "src": "stats",
    "dependency_list": [],
    "tags": ["performance", "Classification"],
    "complexity_class": "linear",
    "metrics": {
        "accuracy": {
            "display_name": "Accuracy",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Accuracy describes how well a model performed at classifying data."
        },
        "balanced_accuracy": {
            "display_name": "Balanced Accuracy",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Balanced Accuracy describes how well a performed by taking the average recall across each class."
        },
        "confusion_matrix": {
            "display_name": "Confusion Matrix",
            "type": "matrix",
            "tags": [],
            "has_range": false,
            "range": null,
            "explanation": "The Confusion Matrix summarizes performance and can highlight areas of weakness where incorrect classification is common."
        },
        "f1": {
            "display_name": "F1 Score",
            "type": "vector",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "The F1 score is a weighted average between a models precision and recall scores"
        },
        "f1_avg": {
            "display_name": "Average F1 Score",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "The F1 score is a weighted average between a models precision and recall scores"
        },
        "fp_rate_avg": {
            "display_name": "Average False Positive Rate",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "FP Rate describes what percentage of wrong predictions were false positives."
        },
        "fp_rate": {
            "display_name": "False Positive Rate",
            "type": "vector",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "FP Rate describes what percentage of wrong predictions were false positives."
        },
        "jaccard_score": {
            "display_name": "Jaccard Score",
            "type": "vector",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Jaccard Score measures the similarity of two two sets of data, and returns a result from 0 to 100%."
        },
        "jaccard_score_avg": {
            "display_name": "Average Jaccard Score",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Jaccard Score measures the similarity of two two sets of data, and returns a result from 0 to 100%."
        },
        "precision_score": {
            "display_name": "Precision Score",
            "type": "vector",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Precision Scores indicates a models ability to not label a positive sample as negative."
        },
        "precision_score_avg": {
            "display_name": "Average Precision Score",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Precision Scores indicates a models ability to not label a positive sample as negative."
        },
        "recall_score": {
            "display_name": "Recall Score",
            "type": "vector",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Recall Scores indicates a models ability to classify all positive image samples"
        },
        "recall_score_avg": {
            "display_name": "Average Recall Score",
            "type": "numeric",
            "tags": [],
            "has_range": true,
            "range": [0, 1],
            "explanation": "Recall Scores indicates a models ability to classify all positive image samples"
        }
    }
}